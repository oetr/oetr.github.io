<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2017-02-10 Fr 01:57 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Perceptron Convergence Theorem</title>
<meta name="generator" content="Org mode" />
<meta name="description" content="Repetition of perceptron convergence algorithm from Haykin's book"
 />
<meta name="keywords" content="algorithm, algorithm of the week, learning, machine learning, neural network, perceptron, Racket" />
<link rel='stylesheet' href='/css/site.css' type='text/css'/>
</head>
<body>
<div id="content">
<h1 class="title">Perceptron Convergence Theorem</h1>
<p>
Perceptron is a function with several inputs and one output that linearly combines the input and the internal weights and applies an activation function on the result.
Weights are some values to be trained.
Perceptron is a simplified model of a neuron with any number of inputs and one output.
The activation function is <i>signum</i>, which returns a 1 if the input is greater than zero, and -1 otherwise.
</p>

<p>
<object type="image/svg+xml" data="ltximg/perceptron_2bdc1786eb02c9d9695708a3bb5c6577e533d1c0.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>

<p>
Perceptron can be trained by correcting its weights by a small amount each time when it incorrectly classifies a training example.
</p>

<p>
<object type="image/svg+xml" data="ltximg/perceptron_4774feb95f88f2e2c252ada381c791856c190e4d.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>

<p>
where error is the difference between desired and actual outputs of the perceptron, and <object type="image/svg+xml" data="ltximg/perceptron_7841263d6144069d682261d6e4be45913effe8fe.svg" class="org-svg">
Sorry, your browser does not support SVG.</object> is the learning rate that specifies how much the old weight should be moved and make the error a little bit smaller.
<object type="image/svg+xml" data="ltximg/perceptron_7841263d6144069d682261d6e4be45913effe8fe.svg" class="org-svg">
Sorry, your browser does not support SVG.</object> is typically a value between 0 and 1.0.
</p>

<p>
Now let's train a perceptron to linearly separate a dataset sampled from a two half-moons distribution. Note that this is a reimplementation of experiments presented in [<a href="#Haykin2009NetworksandLearningMachines">1</a>].
Here are some examples of how the distribution looks like.
Each moon has a radius and a width.
The position of the upper half moon is fixed, while the lower half moon can be freely moved around.
</p>


<div id="org6e8214b" class="figure">
<p><img src="../images/data-example.jpg" alt="data-example.jpg" />
</p>
</div>


<p>
The perceptron has 2 inputs: x and y coordinates of the training example.
The dataset contains 5000 training examples randomly sampled from the half-moon distribution.
The training is done using <object type="image/svg+xml" data="ltximg/perceptron_7841263d6144069d682261d6e4be45913effe8fe.svg" class="org-svg">
Sorry, your browser does not support SVG.</object> = 0.3.
The weights are initialized randomly, and here you can see the trained perceptrons with 100 different initial weights:
</p>



<div id="org68b0239" class="figure">
<p><img src="../images/training-01.jpg" alt="training-01.jpg" />
</p>
</div>


<p>
The error rate goes down pretty quickly as more training examples are considered.
Here is a graph showing the error rate with a varying number of training examples.
The curve was produced by averaging over 50 independent trials.
</p>



<div id="orga3a1cb4" class="figure">
<p><img src="../images/error-rate-separable.jpg" alt="error-rate-separable.jpg" />
</p>
</div>


<p>
Perceptron works well in cases where the classes can be linearly separated from each other.
When it's not the case, the perceptron performs poorly.
Here is an example of trained perceptrons over our half-moons distribution where the lower half-moon can be found inside the upper half-moon:
</p>



<div id="org87df10a" class="figure">
<p><img src="../images/nonseparable-01.jpg" alt="nonseparable-01.jpg" />
</p>
</div>


<p>
The error graph shows that increasing the number of training examples doesn't help, and the performance becomes worse.
As <object type="image/svg+xml" data="ltximg/perceptron_58199a5456c6af107f6ae48293c635a1c794b09c.svg" class="org-svg">
Sorry, your browser does not support SVG.</object> increases, more training examples are found inside the wrong half-moon and push the weights away from horizontal position of the decision boundary.
Thus, the error becomes larger.
</p>


<div id="org66def46" class="figure">
<p><img src="../images/error-rate-nonseparable.jpg" alt="error-rate-nonseparable.jpg" />
</p>
</div>


<p>
Implementation of perceptron convergence algorithm in Racket can be found in in my <a href="http://github.com/oetr/AoW/tree/master/0001-Perceptron-convergence-theorem">github repository</a>.
</p>


<div id="outline-container-org780a1fe" class="outline-2">
<h2 id="org780a1fe">References</h2>
<div class="outline-text-2" id="text-org780a1fe">
<div id="bibliography">

<table>

<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Haykin2009NetworksandLearningMachines">1</a>]
</td>
<td class="bibtexitem">
S.&nbsp;Haykin, <em>Networks and Learning Machines</em>.
 Prentice Hall, 3rd&nbsp;ed., 2009.

</td>
</tr>
</table>
</div>

<br><div class='footer'><a href="http://peter-samarin.de">[Peter Samarin]</a></div>
</div>
</div>
</div>
</body>
</html>
