<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2015-09-09 Wed 13:22 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="viewport" content="width=device-width, initial-scale=1" />
<title>Perceptron convergence algorithm</title>
<meta  name="generator" content="Org-mode" />
<meta  name="description" content="Repetition of perceptron convergence algorithm from Haykin's book"
 />
<meta  name="keywords" content="algorithm, algorithm of the week, learning, machine learning, neural network, perceptron, Racket" />
<link href='http://fonts.googleapis.com/css?family=Libre+Baskerville:400,400italic' rel='stylesheet' type='text/css'>
<link rel='stylesheet' href='css/site.css' type='text/css'/>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Perceptron convergence algorithm</h1>
<p>
Perceptron is a function with several inputs and one output that linearly combines the input and the internal weights and applies an activation function on the result.
Weights are some values to be trained.
Perceptron is a simplified model of a neuron with any number of inputs and one output.
The activation function is <i>signum</i>, which returns a 1 if the input is greater than zero, and -1 otherwise.
\[\mbox{y} = \mbox{sign}(\Sigma_{i}\mbox{w}_i*\mbox{x}_i)\]
</p>

<p>
Perceptron can be trained by correcting its weights by a small amount each time when it incorrectly classifies a training example.
</p>

<p>
\[w_{new} = w_{old} + x*\eta*\mbox{error},\]
</p>

<p>
where error is the difference between desired and actual outputs of the perceptron, and \(\eta\) is the learning rate that specifies how much the old weight should be moved and make the error a little bit smaller.
\(\eta\) is typically a value between 0 and 1.0.
</p>

<p>
Now let's train a perceptron to linearly separate a dataset sampled from a two half-moons distribution. Note that this is a reimplementation of experiments presented in [<a href="#Haykin2009NetworksandLearningMachines">1</a>].
Here are some examples of how the distribution looks like.
Each moon has a radius and a width.
The position of the upper half moon is fixed, while the lower half moon can be freely moved around.
</p>


<div id="orgparagraph1" class="figure">
<p><img src="../images/data-example.jpg" alt="data-example.jpg" />
</p>
</div>


<p>
The perceptron has 2 inputs: x and y coordinates of the training example.
The dataset contains 5000 training examples randomly sampled from the half-moon distribution.
The training is done using \(\eta\) = 0.3.
The weights are initialized randomly, and here you can see the trained perceptrons with 100 different initial weights:
</p>



<div id="orgparagraph2" class="figure">
<p><img src="../images/training-01.jpg" alt="training-01.jpg" />
</p>
</div>


<p>
The error rate goes down pretty quickly as more training examples are considered.
Here is a graph showing the error rate with a varying number of training examples.
The curve was produced by averaging over 50 independent trials.
</p>



<div id="orgparagraph3" class="figure">
<p><img src="../images/error-rate-separable.jpg" alt="error-rate-separable.jpg" />
</p>
</div>


<p>
Perceptron works well in cases where the classes can be linearly separated from each other.
When it's not the case, the perceptron performs poorly.
Here is an example of trained perceptrons over our half-moons distribution where the lower half-moon can be found inside the upper half-moon:
</p>



<div id="orgparagraph4" class="figure">
<p><img src="../images/nonseparable-01.jpg" alt="nonseparable-01.jpg" />
</p>
</div>


<p>
The error graph shows that increasing the number of training examples doesn't help, and the performance becomes worse.
As \(n\) increases, more training examples are found inside the wrong half-moon and push the weights away from horizontal position of the decision boundary.
Thus, the error becomes larger.
</p>


<div id="orgparagraph5" class="figure">
<p><img src="../images/error-rate-nonseparable.jpg" alt="error-rate-nonseparable.jpg" />
</p>
</div>


<p>
Implementation of perceptron convergence algorithm in Racket can be found in in my <a href="http://github.com/oetr/AoW/tree/master/0001-Perceptron-convergence-theorem">github repository</a>.
</p>


<div id="outline-container-orgheadline1" class="outline-2">
<h2 id="orgheadline1">References</h2>
<div class="outline-text-2" id="text-orgheadline1">
<div id="bibliography">

<table>

<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Haykin2009NetworksandLearningMachines">1</a>]
</td>
<td class="bibtexitem">
S.&nbsp;Haykin, <em>Networks and Learning Machines</em>.
 Prentice Hall, 3rd&nbsp;ed., 2009.

</td>
</tr>
</table>
</div>

<div class='footer'><a href="http://peter-samarin.de">Peter Samarin</a></div>
</div>
</div>

<div id="outline-container-orgheadline2" class="outline-2">
<h2 id="orgheadline2">Comments</h2>
<div class="outline-text-2" id="text-orgheadline2">
<div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div>
<script type="text/javascript">
  var disqus_shortname = 'oetrgithubcom';
  //var disqus_developer = 1;
  var disqus_identifier = 'http://peter-samarin.de/blog/perceptron.html';
  var disqus_url        = 'http://peter-samarin.de/blog/perceptron.html';

  var disqus_script = 'embed.js';
  (function () {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  }());
</script>
</div>
</div>
</div>
</body>
</html>
